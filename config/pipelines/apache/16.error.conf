# The pipeline process access log from Apache web server. It does:
# - process data from file or http (on localhost:8080)
# - filter with grok andh the commonlog pattern to retrieve data
# - mutate some fields to the right data type
# - set the @timestamp of the event to the timetamp contained in the processed line, then remove the timestamp generated field
# - print the event to output and to file. The file has a name that contains the date of the current event processed.
# - manage error and access logs with different behaviours


input {

    file {
        path => "D:/Andrea/corsi/elasticsearch/programmi/logstash-7.12.0/data/apache/java_errors.log"
        start_position => "beginning"

        codec => multiline {
            pattern => "%{CATALINA_DATESTAMP}"  
            negate => true
            what => "previous"
            auto_flush_interval => 5
        }
    }
}

filter {
    # if http request is done to path /error, the type is error 
    mutate {
        replace => { type => "error"}
    }
    
    # %{LOGLEVEL:level} %{JAVACLASS:class}: {?<msg>.+?(?=(\r\n|\r|\n))}
    grok {
        match => {
            "message" => "%{CATALINA_DATESTAMP:[@metadata][timestamp]} %{LOGLEVEL:level} %{JAVACLASS:class} %{GREEDYDATA:message}"
        }
    }

    date {
        match => [ "[@metadata][timestamp]", "MMM dd, yyyy HH:mm:ss a"]
    }
    

    mutate {
        remove_field => ["headers", "@version", "host"]
    }
}

output {
    
    stdout {
        codec => rubydebug {
            metadata => true
        }
    }
    
}