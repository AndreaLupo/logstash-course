# The pipeline process access log from Apache web server. It does:
# - process data from file or http (on localhost:8080)
# - filter with grok andh the commonlog pattern to retrieve data
# - mutate some fields to the right data type
# - set the @timestamp of the event to the timetamp contained in the processed line, then remove the timestamp generated field
# - print the event to output and to file. The file has a name that contains the date of the current event processed.


input {
    file { 
        path => "D:/Andrea/corsi/elasticsearch/programmi/logstash-7.12.0/data/apache/apache_*.log"
        # start from the beginning of the file and not wait for a new line to be added
        start_position => "beginning"
        # type is added to each event from this input
        type => "access"
    }

}

filter {
    
    mutate {
        replace => { type => "access"}
    }
    grok {
        match => {
            "message" => '%{HTTPD_COMMONLOG} "%{GREEDYDATA:referrer}" "%{GREEDYDATA:agent}"'
        }
    }

    # if there are errors with grok, delete the event
    if "_grokparsefailure" in [tags] {
        # drop { }
    }

    # parse user agent data, from field agent created by grok to the new field user_agent
    useragent {
        source => "agent"
        target => "user_agent"
    }

    # admin pages - any path containing /admin/ at the beginning
    if [request] =~ /^\/admin\// {
        drop { }
    }

    # static files
    if [request] =~ /^\/js\// 
        or [request] =~ /^\/css\//
        or [request] in ["/robots.txt", "favicon.ico"] {
        drop { }
    }

    # crawlers
    if [useragent][device] == "Spider" {
        drop { }
    }

    mutate {
        # convert the specified fields to the type requested
        convert => {
        "response" => "integer"
        "bytes" => "integer"
        }
    }

    # change the @timestamp value taking the date from the field timestamp
    date {
        match => [
            "timestamp", "dd/MMM/yyyy:HH:mm:ss Z"
        ]
        remove_field => ["timestamp"]
    }

        # clientip is a field generated by the grok pattern
    geoip {
        source => "clientip"
    }
    

    mutate {
        remove_field => ["headers", "@version", "host"]
    }
}

output {
    

    file {
        # file created inside the logstash directory
        path => "%{type}_%{+yyyy_MM_dd}.log"
    }
    
   
    #elasticsearch { 
        # can send data to multiple hosts
    #    hosts => ["localhost:9200"]
        # index => "logstash-%{type}-%{+yyyy_MM_dd}"
    #    http_compression => true
    #}
    stdout {
        codec => rubydebug {
            metadata => true
        }
    }
}