# The pipeline process access log from Apache web server. It does:
# - process data from file or http (on localhost:8080)
# - filter with grok andh the commonlog pattern to retrieve data
# - mutate some fields to the right data type
# - set the @timestamp of the event to the timetamp contained in the processed line, then remove the timestamp generated field
# - print the event to output and to file. The file has a name that contains the date of the current event processed.
# - manage error and access logs with different behaviours


input {
    file { 
        path => "D:/Andrea/corsi/elasticsearch/programmi/logstash-7.12.0/data/apache/apache_*.log"
        # start from the beginning of the file and not wait for a new line to be added
        start_position => "beginning"
        # type is added to each event from this input
        type => "access"
    }

    http {
        type => "access"
    }
}

filter {
    # if http request is done to path /error, the type is error 
    if[headers][request_path] =~ "error" or [path] =~ "errors" {
        mutate {
            replace => { type => "error"}
        }
    } else {
        mutate {
            replace => { type => "access"}
        }
        grok {
            match => {
                "message" => '%{HTTPD_COMMONLOG} "%{GREEDYDATA:referrer}" "%{GREEDYDATA:agent}"'
            }
        }

        # if there are errors with grok, delete the event
        if "_grokparsefailure" in [tags] {
            # drop { }
        }

        # parse user agent data, from field agent created by grok to the new field user_agent
        useragent {
            source => "agent"
            target => "user_agent"
        }

        # admin pages - any path containing /admin/ at the beginning
        if [request] =~ /^\/admin\// {
            drop { }
        }

        # static files
        if [request] =~ /^\/js\// 
            or [request] =~ /^\/css\//
            or [request] in ["/robots.txt", "favicon.ico"] {
            drop { }
        }

        # crawlers
        if [useragent][device] == "Spider" {
            drop { }
        }

        mutate {
            # convert the specified fields to the type requested
            convert => {
            "response" => "integer"
            "bytes" => "integer"
            }
        }

        # change the @timestamp value taking the date from the field timestamp
        date {
            match => [
                "timestamp", "dd/MMM/yyyy:HH:mm:ss Z"
            ]
            remove_field => ["timestamp"]
        }
    }

    # clientip is a field generated by the grok pattern
    geoip {
        source => "clientip"
    }

    mutate {
        remove_field => ["headers", "@version", "host"]
    }
}

output {
    stdout {
        codec => rubydebug
    }

    file {
        # file created inside the logstash directory
        path => "%{type}_%{+yyyy_MM_dd}.log"
    }

    elasticsearch {
        # can send data to multiple hosts
        hosts => ["localhost:9200"]
        # index => "logstash-%{type}-%{+yyyy_MM_dd}"
        http_compression => true
    }
}