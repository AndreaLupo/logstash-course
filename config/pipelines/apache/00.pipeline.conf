# The pipeline process access log from Apache web server. It does:
# - process data from file or http (on localhost:8080)
# - filter with grok andh the commonlog pattern to retrieve data
# - mutate some fields to the right data type
# - set the @timestamp of the event to the timetamp contained in the processed line, then remove the timestamp generated field
# - print the event to output and to file. The file has a name that contains the date of the current event processed.


input {
    file { 
        path => "D:/Andrea/corsi/elasticsearch/programmi/logstash-7.12.0/data/apache/apache_access.log"
        # start from the beginning of the file and not wait for a new line to be added
        start_position => "beginning"
        # type is added to each event from this input
        type => "access"
    }

    http {
        type => "access"
    }
}

filter {

    if[headers][request_path] =~ "error" {
        mutate {
            replace => { type => "error"}
        }
    } else {
        mutate {
            replace => { type => "access"}
        }
        grok {
            match => {
                "message" => '%{HTTPD_COMMONLOG} "%{GREEDYDATA:referrer}" "%{GREEDYDATA:agent}"'
            }
        }

        mutate {
            # convert the specified fields to the type requested
            convert => {
            "response" => "integer"
            "bytes" => "integer"
            }
        }

        # change the @timestamp value taking the date from the field timestamp
        date {
            match => [
                "timestamp", "dd/MMM/yyyy:HH:mm:ss Z"
            ]
            remove_field => ["timestamp"]
        }
    }

    
}

output {
    stdout {
        codec => rubydebug
    }

    file {
        # file created inside the logstash directory
        path => "%{type}_%{+yyyy_MM_dd}.log"
    }
}